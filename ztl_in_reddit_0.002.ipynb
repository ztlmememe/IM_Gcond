{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b6a4e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.062830Z",
     "start_time": "2023-07-07T02:32:59.176098Z"
    }
   },
   "outputs": [],
   "source": [
    "from deeprobust.graph.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils_graphsaint import DataGraphSAINT\n",
    "# watch -n 1 nvidia-smi 4,5\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=6, help='gpu id') # gpu编号\n",
    "parser.add_argument('--dataset', type=str, default='reddit')\n",
    "parser.add_argument('--dis_metric', type=str, default='ztl')\n",
    "parser.add_argument('--epochs', type=int, default=600)\n",
    "parser.add_argument('--nlayers', type=int, default=2)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=1e-4)\n",
    "parser.add_argument('--lr_feat', type=float, default=1e-4)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=0.002)\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=1)\n",
    "parser.add_argument('--inner', type=int, default=1)\n",
    "parser.add_argument('--outer', type=int, default=10)\n",
    "parser.add_argument('--option', type=int, default=0)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--label_rate', type=float, default=1)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# import pickle\n",
    "# with open('D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/label.pkl', 'rb') as f:\n",
    "#     info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c436a832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.110704Z",
     "start_time": "2023-07-07T02:33:04.064828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu_id=6, dataset='reddit', dis_metric='ztl', epochs=600, nlayers=2, hidden=256, lr_adj=0.0001, lr_feat=0.0001, lr_model=0.01, weight_decay=0.0, dropout=0.0, normalize_features=True, keep_ratio=1.0, reduction_rate=0.002, seed=1, alpha=0, debug=0, sgc=1, inner=1, outer=10, option=0, save=0, label_rate=1, one_step=0)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# random seed setting\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "data_graphsaint = ['flickr', 'reddit', 'ogbn-arxiv']\n",
    "if args.dataset in data_graphsaint:\n",
    "    data = DataGraphSAINT(args.dataset)\n",
    "    data_full = data.data_full\n",
    "else:\n",
    "    data_full = get_dataset(args.dataset, args.normalize_features)\n",
    "    data = Transd2Ind(data_full, keep_ratio=args.keep_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c888ea80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.197861Z",
     "start_time": "2023-07-07T02:33:04.112699Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "# from utils import match_loss, regularization, row_normalize_tensor\n",
    "from utils import regularization, row_normalize_tensor\n",
    "import deeprobust.graph.utils as utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.gcn import GCN\n",
    "from models.sgc import SGC\n",
    "from models.sgc_multi import SGC as SGC1\n",
    "from models.parametrized_adj import PGE\n",
    "import scipy.sparse as sp\n",
    "from torch_sparse import SparseTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def combined_distance(gwr, gws, alpha=0.3, beta=0.7): # 更改权重\n",
    "    shape = gwr.shape\n",
    "\n",
    "    # TODO: output node!!!!\n",
    "    if len(gwr.shape) == 2:\n",
    "        gwr = gwr.T\n",
    "        gws = gws.T\n",
    "\n",
    "            # 第二点:正则化\n",
    "        gwr = F.normalize(gwr, p=2, dim=-1) \n",
    "        gws = F.normalize(gws, p=2, dim=-1)\n",
    "\n",
    "    if len(shape) == 4: # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2: # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return 0\n",
    "    \n",
    "    # 余弦相似度\n",
    "    cosine_sim = F.cosine_similarity(gwr, gws, dim=-1)\n",
    "    \n",
    "    # 第三点:更高级的距离度量,这里使用MMD距离\n",
    "    XX = torch.matmul(gwr, gwr.t())\n",
    "    YY = torch.matmul(gws, gws.t())\n",
    "    XY = torch.matmul(gwr, gws.t())\n",
    "    \n",
    "    mmd = torch.mean(XX) - 2 * torch.mean(XY) + torch.mean(YY)\n",
    "    \n",
    "    # 线性组合距离\n",
    "    distance = alpha * (1 - cosine_sim) + beta * mmd\n",
    "    \n",
    "    return torch.sum(distance)\n",
    "    # cosine_similarity = F.cosine_similarity(gwr, gws, dim=-1)\n",
    "    # euclidean_distance = torch.norm(gwr - gws, dim=-1)\n",
    "\n",
    "    # # 归一化余弦相似度和欧几里德范数的距离线性组合\n",
    "    # distance = alpha * (1 - cosine_similarity) + beta * euclidean_distance\n",
    "\n",
    "    # return torch.sum(distance)\n",
    "\n",
    "\n",
    "def match_loss(gw_syn, gw_real, args, device):\n",
    "\n",
    "    dis = torch.tensor(0.0).to(device)  # 初始化匹配损失\n",
    "\n",
    "    if args.dis_metric == 'ztl':  # 使用自定义的距离度量函数\n",
    "\n",
    "        for ig in range(len(gw_real)):\n",
    "            gwr = gw_real[ig]\n",
    "            gws = gw_syn[ig]\n",
    "            dis += combined_distance(gwr, gws)\n",
    "    else:\n",
    "        exit('DC error: unknown distance function')  # 如果距离度量函数未知，则抛出错误\n",
    "\n",
    "    return dis\n",
    "\n",
    "class GCond:\n",
    "\n",
    "    def __init__(self, data, args, device='cuda', **kwargs):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "        # n = data.nclass * args.nsamples\n",
    "        n = int(data.feat_train.shape[0] * args.reduction_rate)\n",
    "        # from collections import Counter; print(Counter(data.labels_train))\n",
    "\n",
    "        d = data.feat_train.shape[1]\n",
    "        self.nnodes_syn = n\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=device,args=args).to(device)\n",
    "\n",
    "        self.labels_syn = torch.LongTensor(self.generate_labels_syn(data)).to(device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def generate_labels_syn(self, data):\n",
    "        from collections import Counter\n",
    "        counter = Counter(data.labels_train)\n",
    "        num_class_dict = {}\n",
    "        n = len(data.labels_train)\n",
    "\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x:x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []\n",
    "        self.syn_class_indices = {}\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * self.args.reduction_rate) - sum_\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * self.args.reduction_rate), 1)\n",
    "                sum_ += num_class_dict[c]\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        self.num_class_dict = num_class_dict\n",
    "        return labels_syn\n",
    "\n",
    "\n",
    "    def test_with_val(self, verbose=True):\n",
    "        res = []\n",
    "\n",
    "        data, device = self.data, self.device\n",
    "        feat_syn, pge, labels_syn = self.feat_syn.detach(), \\\n",
    "                                self.pge, self.labels_syn\n",
    "\n",
    "        # with_bn = True if args.dataset in ['ogbn-arxiv'] else False\n",
    "        model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                    weight_decay=5e-4, nlayers=2,\n",
    "                    nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        if self.args.dataset in ['ogbn-arxiv']:\n",
    "            model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                        weight_decay=0e-4, nlayers=2, with_bn=False,\n",
    "                        nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        adj_syn = pge.inference(feat_syn)\n",
    "        args = self.args\n",
    "\n",
    "        if self.args.save:\n",
    "            torch.save(adj_syn, f'saved_ours/adj_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "            torch.save(feat_syn, f'saved_ours/feat_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "\n",
    "        if self.args.lr_adj == 0:\n",
    "            n = len(labels_syn)\n",
    "            adj_syn = torch.zeros((n, n))\n",
    "\n",
    "        model.fit_with_val(feat_syn, adj_syn, labels_syn, data,\n",
    "                     train_iters=600, normalize=True, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        labels_test = torch.LongTensor(data.labels_test).cuda()\n",
    "\n",
    "        labels_train = torch.LongTensor(data.labels_train).cuda()\n",
    "        output = model.predict(data.feat_train, data.adj_train)\n",
    "        loss_train = F.nll_loss(output, labels_train)\n",
    "        acc_train = utils.accuracy(output, labels_train)\n",
    "        if verbose:\n",
    "            print(\"Train set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_train.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_train.item()))\n",
    "        res.append(acc_train.item())\n",
    "\n",
    "        # Full graph\n",
    "        output = model.predict(data.feat_full, data.adj_full)\n",
    "        loss_test = F.nll_loss(output[data.idx_test], labels_test)\n",
    "        acc_test = utils.accuracy(output[data.idx_test], labels_test)\n",
    "        res.append(acc_test.item())\n",
    "        if verbose:\n",
    "            print(\"Test set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_test.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return res\n",
    "\n",
    "    def train(self, verbose=True):\n",
    "\n",
    "        # 创建空列表用于保存每个epoch的LOSS和准确率\n",
    "        train_loss_values = []\n",
    "        train_loss_inter_values = []\n",
    "        train_acc_values = []\n",
    "        val_loss_values = []\n",
    "        val_acc_values = []\n",
    "\n",
    "        args = self.args\n",
    "        data = self.data\n",
    "        feat_syn, pge, labels_syn = self.feat_syn, self.pge, self.labels_syn\n",
    "        features, adj, labels = data.feat_full, data.adj_full, data.labels_full\n",
    "        idx_train = data.idx_train\n",
    "\n",
    "        syn_class_indices = self.syn_class_indices\n",
    "\n",
    "        features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "\n",
    "        feat_sub, adj_sub = self.get_sub_adj_feat(features)\n",
    "        self.feat_syn.data.copy_(feat_sub)\n",
    "\n",
    "        if utils.is_sparse_tensor(adj):\n",
    "            adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "        else:\n",
    "            adj_norm = utils.normalize_adj_tensor(adj)\n",
    "\n",
    "        adj = adj_norm\n",
    "        adj = SparseTensor(row=adj._indices()[0], col=adj._indices()[1],\n",
    "                value=adj._values(), sparse_sizes=adj.size()).t()\n",
    "\n",
    "\n",
    "        outer_loop, inner_loop = get_loops(args)\n",
    "        loss_avg = 0\n",
    "        # loss_inter_avg = 0\n",
    "\n",
    "        for it in range(args.epochs+1):\n",
    "            if args.dataset in ['ogbn-arxiv']:\n",
    "                model = SGC1(nfeat=feat_syn.shape[1], nhid=self.args.hidden,\n",
    "                            dropout=0.0, with_bn=False,\n",
    "                            weight_decay=0e-4, nlayers=2,\n",
    "                            nclass=data.nclass,\n",
    "                            device=self.device).to(self.device)\n",
    "            else:\n",
    "                if args.sgc == 1:\n",
    "                    model = SGC(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout,\n",
    "                                nlayers=args.nlayers, with_bn=False,\n",
    "                                device=self.device).to(self.device)\n",
    "                else:\n",
    "                    model = GCN(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout, nlayers=args.nlayers,\n",
    "                                device=self.device).to(self.device)\n",
    "\n",
    "\n",
    "            model.initialize()\n",
    "\n",
    "            model_parameters = list(model.parameters())\n",
    "\n",
    "            optimizer_model = torch.optim.Adam(model_parameters, lr=args.lr_model)\n",
    "            model.train()\n",
    "            \n",
    "            for ol in range(outer_loop):\n",
    "                adj_syn = pge(self.feat_syn)\n",
    "                adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)\n",
    "                feat_syn_norm = feat_syn\n",
    "\n",
    "                BN_flag = False\n",
    "                for module in model.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    model.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = model.forward(features, adj_norm)\n",
    "                    for module in model.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                loss = torch.tensor(0.0).to(self.device)\n",
    "                \n",
    "                for c in range(data.nclass):\n",
    "                    batch_size, n_id, adjs = data.retrieve_class_sampler(\n",
    "                            c, adj, transductive=True, args=args)\n",
    "                    if args.nlayers == 1:\n",
    "                        adjs = [adjs]\n",
    "\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    output = model.forward_sampler(features[n_id], adjs)\n",
    "                    loss_real = F.nll_loss(output, labels[n_id[:batch_size]])\n",
    "\n",
    "                    gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "                    \n",
    "\n",
    "                    output_syn = model.forward(feat_syn, adj_syn_norm)\n",
    "\n",
    "                    ind = syn_class_indices[c]\n",
    "                    loss_syn = F.nll_loss(\n",
    "                            output_syn[ind[0]: ind[1]],\n",
    "                            labels_syn[ind[0]: ind[1]])\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "\n",
    "\n",
    "                    coeff = self.num_class_dict[c] / max(self.num_class_dict.values())\n",
    "                    loss += coeff  * match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "\n",
    "#                 print(gw_real_norm/c)\n",
    "                # gws.append(gw_real_norm/c)\n",
    "                loss_avg += loss.item()\n",
    "\n",
    "                # TODO: regularize\n",
    "                if args.alpha > 0:\n",
    "                    loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(labels_syn))\n",
    "                else:\n",
    "                    loss_reg = torch.tensor(0)\n",
    "\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "                # update sythetic graph\n",
    "                self.optimizer_feat.zero_grad()\n",
    "                self.optimizer_pge.zero_grad()\n",
    "                loss.backward()\n",
    "                if it % 50 < 10:\n",
    "                    self.optimizer_pge.step()\n",
    "                else:\n",
    "                    self.optimizer_feat.step()\n",
    "\n",
    "                if args.debug and ol % 5 ==0:\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                if ol == outer_loop - 1:\n",
    "                    # print('loss_reg:', loss_reg.item())\n",
    "                    # print('Gradient matching loss:', loss.item())\n",
    "                    break\n",
    "\n",
    "                feat_syn_inner = feat_syn.detach()\n",
    "                adj_syn_inner = pge.inference(feat_syn_inner)\n",
    "                adj_syn_inner_norm = utils.normalize_adj_tensor(adj_syn_inner, sparse=False)\n",
    "                feat_syn_inner_norm = feat_syn_inner\n",
    "                for j in range(inner_loop):\n",
    "                    optimizer_model.zero_grad()\n",
    "                    output_syn_inner = model.forward(feat_syn_inner_norm, adj_syn_inner_norm)\n",
    "                    loss_syn_inner = F.nll_loss(output_syn_inner, labels_syn)\n",
    "                    loss_syn_inner.backward()\n",
    "                    # print(loss_syn_inner.item())\n",
    "                    optimizer_model.step() # update gnn param\n",
    "            loss_avg /= (data.nclass*outer_loop)\n",
    "\n",
    "            train_loss_values.append(loss_avg)\n",
    "\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                print('Epoch {}, loss_avg: {}'.format(it, loss_avg))\n",
    "\n",
    "            eval_epochs = [400, 600, 800, 1000, 1200, 1600, 2000, 3000, 4000, 5000]\n",
    "\n",
    "            if verbose and it in eval_epochs:\n",
    "            # if verbose and (it+1) % 50 == 0:\n",
    "                res = []\n",
    "                runs = 1 if args.dataset in ['ogbn-arxiv'] else 3\n",
    "                for i in range(runs):\n",
    "                    if args.dataset in ['ogbn-arxiv']:\n",
    "                        res.append(self.test_with_val())\n",
    "                    else:\n",
    "                        res.append(self.test_with_val())\n",
    "\n",
    "                res = np.array(res)\n",
    "                try:\n",
    "                    train_acc_values.append(res.mean(0)[0])  # 假设train_acc是训练准确率的变量\n",
    "                    val_acc_values.append(res.mean(0)[1])  # 假设val_loss是验证LOSS的变量\n",
    "                except:\n",
    "                    pass\n",
    "                # 这段代码是在输出训练或测试的平均准确率和标准差,准确率越高越好，标准差越小越好\n",
    "                print('Train/Test Mean Accuracy:',\n",
    "                        repr([res.mean(0), res.std(0)]))\n",
    "                # 输出示例：[array([0.91666667, 0.794 ]), array([0.02357023, 0.0008165 ])]\n",
    "                # 这个输出表明训练或测试的平均准确率为 0.91666667，标准差为 0.02357023，测试的平均准确率为 0.794，标准差为 0.0008165。\n",
    "                # 也就是说每个array的0是训练集，1是测试集，第一个array是平均准确率，第二个array是标准差\n",
    "                \n",
    "                # repr() 函数是将对象转化为供解释器读取的形式（即字符串形式），它通常用于调试和测试中，方便查看对象的值。\n",
    "\n",
    "                # res.mean(0) 计算 res 中每一列的平均值，返回一个一维张量，表示平均准确率。res.std(0) 则计算每一列的标准差，也返回一个一维张量，表示准确率的标准差。\n",
    "\n",
    "                # 最终输出的结果是一个二元组，第一个元素是平均准确率的一维张量，第二个元素是标准差的一维张量，它们都用 repr() 函数转换成字符串形式。\n",
    "        # 绘制LOSS变化情况的图像\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.plot(train_loss_values, label='Train Loss')\n",
    "        plt.plot(val_loss_values, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # 绘制准确率变化情况的图像\n",
    "        plt.figure()\n",
    "        plt.plot(train_acc_values, label='Train Accuracy')\n",
    "        plt.plot(val_acc_values, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def get_sub_adj_feat(self, features):\n",
    "        data = self.data\n",
    "        args = self.args\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter;\n",
    "        counter = Counter(self.labels_syn.cpu().numpy())\n",
    "\n",
    "        for c in range(data.nclass):\n",
    "            tmp = data.retrieve_class(c, num=counter[c])\n",
    "            tmp = list(tmp)\n",
    "            idx_selected = idx_selected + tmp\n",
    "        idx_selected = np.array(idx_selected).reshape(-1)\n",
    "        features = features[self.data.idx_train][idx_selected]\n",
    "\n",
    "        # adj_knn = torch.zeros((data.nclass*args.nsamples, data.nclass*args.nsamples)).to(self.device)\n",
    "        # for i in range(data.nclass):\n",
    "        #     idx = np.arange(i*args.nsamples, i*args.nsamples+args.nsamples)\n",
    "        #     adj_knn[np.ix_(idx, idx)] = 1\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # features[features!=0] = 1\n",
    "        k = 2\n",
    "        sims = cosine_similarity(features.cpu().numpy())\n",
    "        sims[(np.arange(len(sims)), np.arange(len(sims)))] = 0\n",
    "        for i in range(len(sims)):\n",
    "            indices_argsort = np.argsort(sims[i])\n",
    "            sims[i, indices_argsort[: -k]] = 0\n",
    "        adj_knn = torch.FloatTensor(sims).to(self.device)\n",
    "        return features, adj_knn\n",
    "\n",
    "\n",
    "def get_loops(args):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if args.one_step:\n",
    "        if args.dataset =='ogbn-arxiv':\n",
    "            return 5, 0\n",
    "        return 1, 0\n",
    "    if args.dataset in ['ogbn-arxiv']:\n",
    "        return args.outer, args.inner\n",
    "    if args.dataset in ['cora']:\n",
    "        return 20, 15 # sgc\n",
    "    if args.dataset in ['citeseer']:\n",
    "        return 20, 15\n",
    "    if args.dataset in ['physics']:\n",
    "        return 20, 10\n",
    "    else:\n",
    "        return 20, 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6655786a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T03:39:30.583039Z",
     "start_time": "2023-07-07T02:33:04.199853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_syn: (307, 307) feat_syn: torch.Size([307, 602])\n",
      "[15, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwang/miniconda3/envs/ztl/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss_avg: 6.944514018733327\n",
      "Epoch 50, loss_avg: 5.7534248657536\n",
      "Epoch 100, loss_avg: 5.332331352358878\n",
      "Epoch 150, loss_avg: 3.90410401805324\n",
      "Epoch 200, loss_avg: 2.943757158064248\n",
      "Epoch 250, loss_avg: 2.5385311672834354\n",
      "Epoch 300, loss_avg: 2.2448341348673395\n"
     ]
    }
   ],
   "source": [
    "# from gcond_transduct_multi_level import GCond 479m 628m\n",
    "agent = GCond(data, args, device='cuda:6')\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c04ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
