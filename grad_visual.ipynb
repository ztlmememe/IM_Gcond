{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b6a4e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.062830Z",
     "start_time": "2023-07-07T02:32:59.176098Z"
    }
   },
   "outputs": [],
   "source": [
    "from deeprobust.graph.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils_graphsaint import DataGraphSAINT\n",
    "# watch -n 1 nvidia-smi 4,5\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=4, help='gpu id') # gpu编号\n",
    "parser.add_argument('--dataset', type=str, default='flickr')\n",
    "# parser.add_argument('--dis_metric', type=str, default='ztl')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours')\n",
    "parser.add_argument('--epochs', type=int, default=400)\n",
    "parser.add_argument('--nlayers', type=int, default=2)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.005)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.005)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=0.01)\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=2)\n",
    "parser.add_argument('--inner', type=int, default=1)\n",
    "parser.add_argument('--outer', type=int, default=10)\n",
    "parser.add_argument('--option', type=int, default=0)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--label_rate', type=float, default=1)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# import pickle\n",
    "# with open('D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/label.pkl', 'rb') as f:\n",
    "#     info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c436a832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.110704Z",
     "start_time": "2023-07-07T02:33:04.064828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu_id=4, dataset='flickr', dis_metric='ours', epochs=400, nlayers=2, hidden=256, lr_adj=0.005, lr_feat=0.005, lr_model=0.01, weight_decay=0.0, dropout=0.0, normalize_features=True, keep_ratio=1.0, reduction_rate=0.01, seed=1, alpha=0, debug=0, sgc=2, inner=1, outer=10, option=0, save=0, label_rate=1, one_step=0)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# random seed setting\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "data_graphsaint = ['flickr', 'reddit', 'ogbn-arxiv']\n",
    "if args.dataset in data_graphsaint:\n",
    "    data = DataGraphSAINT(args.dataset)\n",
    "    data_full = data.data_full\n",
    "else:\n",
    "    data_full = get_dataset(args.dataset, args.normalize_features)\n",
    "    data = Transd2Ind(data_full, keep_ratio=args.keep_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c888ea80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.197861Z",
     "start_time": "2023-07-07T02:33:04.112699Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from utils import match_loss, regularization, row_normalize_tensor\n",
    "# from utils import regularization, row_normalize_tensor\n",
    "import deeprobust.graph.utils as utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.gcn import GCN\n",
    "from models.sgc import SGC\n",
    "from models.sgc_multi import SGC as SGC1\n",
    "from models.parametrized_adj import PGE\n",
    "import scipy.sparse as sp\n",
    "from torch_sparse import SparseTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# def combined_distance(gwr, gws, alpha=0.2, beta=0.8):\n",
    "#     shape = gwr.shape\n",
    "\n",
    "#     # TODO: output node!!!!\n",
    "#     if len(gwr.shape) == 2:\n",
    "#         gwr = gwr.T\n",
    "#         gws = gws.T\n",
    "\n",
    "#     if len(shape) == 4: # conv, out*in*h*w\n",
    "#         gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "#         gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "#     elif len(shape) == 3:  # layernorm, C*h*w\n",
    "#         gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "#         gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "#     elif len(shape) == 2: # linear, out*in\n",
    "#         tmp = 'do nothing'\n",
    "#     elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "#         gwr = gwr.reshape(1, shape[0])\n",
    "#         gws = gws.reshape(1, shape[0])\n",
    "#         return 0\n",
    "\n",
    "#     cosine_similarity = F.cosine_similarity(gwr, gws, dim=-1)\n",
    "#     euclidean_distance = torch.norm(gwr - gws, dim=-1)\n",
    "\n",
    "#     # 归一化余弦相似度和欧几里德范数的距离线性组合\n",
    "#     distance = alpha * (1 - cosine_similarity) + beta * euclidean_distance\n",
    "#     return torch.sum(distance)\n",
    "\n",
    "    #  # 计算距离度量方法改进\n",
    "    # cosine_distance = 1 - F.cosine_similarity(gwr, gws, dim=-1)  # 使用余弦相似度计算距离\n",
    "    # euclidean_distance = torch.norm(gwr - gws, dim=-1)  # 使用欧几里德范数计算距离\n",
    "\n",
    "    # # 距离度量方法改进：通过归一化距离来综合衡量相似性和差异性\n",
    "    # normalized_distance = (cosine_distance / (cosine_distance + euclidean_distance + 1e-8)).mean()\n",
    "\n",
    "    \n",
    "    # return normalized_distance\n",
    "\n",
    "\n",
    "# def match_loss(gw_syn, gw_real, args, device):\n",
    "\n",
    "#     dis = torch.tensor(0.0).to(device)  # 初始化匹配损失\n",
    "\n",
    "#     if args.dis_metric == 'ztl':  # 使用自定义的距离度量函数\n",
    "\n",
    "#         for ig in range(len(gw_real)):\n",
    "#             gwr = gw_real[ig]\n",
    "#             gws = gw_syn[ig]\n",
    "#             dis += combined_distance(gwr, gws)\n",
    "#     else:\n",
    "#         exit('DC error: unknown distance function')  # 如果距离度量函数未知，则抛出错误\n",
    "\n",
    "#     return dis\n",
    "\n",
    "class GCond:\n",
    "\n",
    "    def __init__(self, data, args, device='cuda', **kwargs):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "        # n = data.nclass * args.nsamples\n",
    "        n = int(data.feat_train.shape[0] * args.reduction_rate)\n",
    "        # from collections import Counter; print(Counter(data.labels_train))\n",
    "\n",
    "        d = data.feat_train.shape[1]\n",
    "        self.nnodes_syn = n\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=device,args=args).to(device)\n",
    "\n",
    "        self.labels_syn = torch.LongTensor(self.generate_labels_syn(data)).to(device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def generate_labels_syn(self, data):\n",
    "        from collections import Counter\n",
    "        counter = Counter(data.labels_train)\n",
    "        num_class_dict = {}\n",
    "        n = len(data.labels_train)\n",
    "\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x:x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []\n",
    "        self.syn_class_indices = {}\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * self.args.reduction_rate) - sum_\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * self.args.reduction_rate), 1)\n",
    "                sum_ += num_class_dict[c]\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        self.num_class_dict = num_class_dict\n",
    "        return labels_syn\n",
    "\n",
    "\n",
    "    def test_with_val(self, verbose=True):\n",
    "        res = []\n",
    "\n",
    "        data, device = self.data, self.device\n",
    "        feat_syn, pge, labels_syn = self.feat_syn.detach(), \\\n",
    "                                self.pge, self.labels_syn\n",
    "\n",
    "        # with_bn = True if args.dataset in ['ogbn-arxiv'] else False\n",
    "        model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                    weight_decay=5e-4, nlayers=2,\n",
    "                    nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        if self.args.dataset in ['ogbn-arxiv']:\n",
    "            model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                        weight_decay=0e-4, nlayers=2, with_bn=False,\n",
    "                        nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        adj_syn = pge.inference(feat_syn)\n",
    "        args = self.args\n",
    "\n",
    "        if self.args.save:\n",
    "            torch.save(adj_syn, f'saved_ours/adj_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "            torch.save(feat_syn, f'saved_ours/feat_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "\n",
    "        if self.args.lr_adj == 0:\n",
    "            n = len(labels_syn)\n",
    "            adj_syn = torch.zeros((n, n))\n",
    "\n",
    "        model.fit_with_val(feat_syn, adj_syn, labels_syn, data,\n",
    "                     train_iters=600, normalize=True, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        labels_test = torch.LongTensor(data.labels_test).cuda()\n",
    "\n",
    "        labels_train = torch.LongTensor(data.labels_train).cuda()\n",
    "        output = model.predict(data.feat_train, data.adj_train)\n",
    "        loss_train = F.nll_loss(output, labels_train)\n",
    "        acc_train = utils.accuracy(output, labels_train)\n",
    "        if verbose:\n",
    "            print(\"Train set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_train.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_train.item()))\n",
    "        res.append(acc_train.item())\n",
    "\n",
    "        # Full graph\n",
    "        output = model.predict(data.feat_full, data.adj_full)\n",
    "        loss_test = F.nll_loss(output[data.idx_test], labels_test)\n",
    "        acc_test = utils.accuracy(output[data.idx_test], labels_test)\n",
    "        res.append(acc_test.item())\n",
    "        if verbose:\n",
    "            print(\"Test set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_test.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return res\n",
    "\n",
    "    def train(self, verbose=True):\n",
    "\n",
    "        # 创建空列表用于保存每个epoch的LOSS和准确率\n",
    "        train_loss_values = []\n",
    "        train_loss_inter_values = []\n",
    "        train_acc_values = []\n",
    "        val_loss_values = []\n",
    "        val_acc_values = []\n",
    "\n",
    "        args = self.args\n",
    "        data = self.data\n",
    "        feat_syn, pge, labels_syn = self.feat_syn, self.pge, self.labels_syn\n",
    "        features, adj, labels = data.feat_full, data.adj_full, data.labels_full\n",
    "        idx_train = data.idx_train\n",
    "\n",
    "        syn_class_indices = self.syn_class_indices\n",
    "\n",
    "        features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "\n",
    "        feat_sub, adj_sub = self.get_sub_adj_feat(features)\n",
    "        self.feat_syn.data.copy_(feat_sub)\n",
    "\n",
    "        if utils.is_sparse_tensor(adj):\n",
    "            adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "        else:\n",
    "            adj_norm = utils.normalize_adj_tensor(adj)\n",
    "\n",
    "        adj = adj_norm\n",
    "        adj = SparseTensor(row=adj._indices()[0], col=adj._indices()[1],\n",
    "                value=adj._values(), sparse_sizes=adj.size()).t()\n",
    "\n",
    "\n",
    "        outer_loop, inner_loop = get_loops(args)\n",
    "        loss_avg = 0\n",
    "        # loss_inter_avg = 0\n",
    "        # gw_reals = []\n",
    "        gw_syns = []\n",
    "        for it in range(args.epochs+1):\n",
    "            if args.dataset in ['ogbn-arxiv']:\n",
    "                model = SGC1(nfeat=feat_syn.shape[1], nhid=self.args.hidden,\n",
    "                            dropout=0.0, with_bn=False,\n",
    "                            weight_decay=0e-4, nlayers=2,\n",
    "                            nclass=data.nclass,\n",
    "                            device=self.device).to(self.device)\n",
    "            else:\n",
    "                if args.sgc == 1:\n",
    "                    model = SGC(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout,\n",
    "                                nlayers=args.nlayers, with_bn=False,\n",
    "                                device=self.device).to(self.device)\n",
    "                else:\n",
    "                    model = GCN(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout, nlayers=args.nlayers,\n",
    "                                device=self.device).to(self.device)\n",
    "\n",
    "\n",
    "            model.initialize()\n",
    "\n",
    "            model_parameters = list(model.parameters())\n",
    "\n",
    "            optimizer_model = torch.optim.Adam(model_parameters, lr=args.lr_model)\n",
    "            model.train()\n",
    "            \n",
    "            for ol in range(outer_loop):\n",
    "                adj_syn = pge(self.feat_syn)\n",
    "                adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)\n",
    "                feat_syn_norm = feat_syn\n",
    "\n",
    "                BN_flag = False\n",
    "                for module in model.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    model.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = model.forward(features, adj_norm)\n",
    "                    for module in model.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                loss = torch.tensor(0.0).to(self.device)\n",
    "                \n",
    "                for c in range(data.nclass):\n",
    "                    batch_size, n_id, adjs = data.retrieve_class_sampler(\n",
    "                            c, adj, transductive=True, args=args)\n",
    "                    if args.nlayers == 1:\n",
    "                        adjs = [adjs]\n",
    "\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    output = model.forward_sampler(features[n_id], adjs)\n",
    "                    loss_real = F.nll_loss(output, labels[n_id[:batch_size]])\n",
    "\n",
    "                    gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "                    # gw_reals.append(gw_real)\n",
    "                    \n",
    "\n",
    "                    output_syn = model.forward(feat_syn, adj_syn_norm)\n",
    "\n",
    "                    ind = syn_class_indices[c]\n",
    "                    loss_syn = F.nll_loss(\n",
    "                            output_syn[ind[0]: ind[1]],\n",
    "                            labels_syn[ind[0]: ind[1]])\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                    \n",
    "                    coeff = self.num_class_dict[c] / max(self.num_class_dict.values())\n",
    "                    loss += coeff  * match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "                    for i in range(len(model_parameters)):\n",
    "                        gw_syn[i].norm(2).cpu().detach().numpy()\n",
    "\n",
    "                    gw_syns.append(gw_syn)\n",
    "\n",
    "#                 print(gw_real_norm/c)\n",
    "                # gws.append(gw_real_norm/c)\n",
    "                loss_avg += loss.item()\n",
    "\n",
    "                # TODO: regularize\n",
    "                if args.alpha > 0:\n",
    "                    loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(labels_syn))\n",
    "                else:\n",
    "                    loss_reg = torch.tensor(0)\n",
    "\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "                # update sythetic graph\n",
    "                self.optimizer_feat.zero_grad()\n",
    "                self.optimizer_pge.zero_grad()\n",
    "                loss.backward()\n",
    "                if it % 50 < 10:\n",
    "                    self.optimizer_pge.step()\n",
    "                else:\n",
    "                    self.optimizer_feat.step()\n",
    "\n",
    "                if args.debug and ol % 5 ==0:\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                if ol == outer_loop - 1:\n",
    "                    # print('loss_reg:', loss_reg.item())\n",
    "                    # print('Gradient matching loss:', loss.item())\n",
    "                    break\n",
    "\n",
    "                feat_syn_inner = feat_syn.detach()\n",
    "                adj_syn_inner = pge.inference(feat_syn_inner)\n",
    "                adj_syn_inner_norm = utils.normalize_adj_tensor(adj_syn_inner, sparse=False)\n",
    "                feat_syn_inner_norm = feat_syn_inner\n",
    "                for j in range(inner_loop):\n",
    "                    optimizer_model.zero_grad()\n",
    "                    output_syn_inner = model.forward(feat_syn_inner_norm, adj_syn_inner_norm)\n",
    "                    loss_syn_inner = F.nll_loss(output_syn_inner, labels_syn)\n",
    "                    loss_syn_inner.backward()\n",
    "                    # print(loss_syn_inner.item())\n",
    "                    optimizer_model.step() # update gnn param\n",
    "            loss_avg /= (data.nclass*outer_loop)\n",
    "\n",
    "            train_loss_values.append(loss_avg)\n",
    "\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                print('Epoch {}, loss_avg: {}'.format(it, loss_avg))\n",
    "\n",
    "            eval_epochs = [400, 600, 800, 1000, 1200, 1600, 2000, 3000, 4000, 5000]\n",
    "\n",
    "            if verbose and it in eval_epochs:\n",
    "            # if verbose and (it+1) % 50 == 0:\n",
    "                res = []\n",
    "                runs = 1 if args.dataset in ['ogbn-arxiv'] else 3\n",
    "                for i in range(runs):\n",
    "                    if args.dataset in ['ogbn-arxiv']:\n",
    "                        res.append(self.test_with_val())\n",
    "                    else:\n",
    "                        res.append(self.test_with_val())\n",
    "\n",
    "                res = np.array(res)\n",
    "                try:\n",
    "                    train_acc_values.append(res.mean(0)[0])  # 假设train_acc是训练准确率的变量\n",
    "                    val_acc_values.append(res.mean(0)[1])  # 假设val_loss是验证LOSS的变量\n",
    "                except:\n",
    "                    pass\n",
    "                # 这段代码是在输出训练或测试的平均准确率和标准差,准确率越高越好，标准差越小越好\n",
    "                print('Train/Test Mean Accuracy:',\n",
    "                        repr([res.mean(0), res.std(0)]))\n",
    "                # 输出示例：[array([0.91666667, 0.794 ]), array([0.02357023, 0.0008165 ])]\n",
    "                # 这个输出表明训练或测试的平均准确率为 0.91666667，标准差为 0.02357023，测试的平均准确率为 0.794，标准差为 0.0008165。\n",
    "                # 也就是说每个array的0是训练集，1是测试集，第一个array是平均准确率，第二个array是标准差\n",
    "                \n",
    "                # repr() 函数是将对象转化为供解释器读取的形式（即字符串形式），它通常用于调试和测试中，方便查看对象的值。\n",
    "\n",
    "                # res.mean(0) 计算 res 中每一列的平均值，返回一个一维张量，表示平均准确率。res.std(0) 则计算每一列的标准差，也返回一个一维张量，表示准确率的标准差。\n",
    "\n",
    "                # 最终输出的结果是一个二元组，第一个元素是平均准确率的一维张量，第二个元素是标准差的一维张量，它们都用 repr() 函数转换成字符串形式。\n",
    "        # 绘制LOSS变化情况的图像\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.plot(train_loss_values, label='Train Loss')\n",
    "        plt.plot(val_loss_values, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # 绘制准确率变化情况的图像\n",
    "        plt.figure()\n",
    "        plt.plot(train_acc_values, label='Train Accuracy')\n",
    "        plt.plot(val_acc_values, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #         # 计算每个参数的梯度范数\n",
    "        # param_norms = [[grad[i].norm(2) for grad in gw_reals] for i in range(len(model_parameters))]\n",
    "\n",
    "        # param_norms = gw_syns\n",
    "        return gw_syns\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_sub_adj_feat(self, features):\n",
    "        data = self.data\n",
    "        args = self.args\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter;\n",
    "        counter = Counter(self.labels_syn.cpu().numpy())\n",
    "\n",
    "        for c in range(data.nclass):\n",
    "            tmp = data.retrieve_class(c, num=counter[c])\n",
    "            tmp = list(tmp)\n",
    "            idx_selected = idx_selected + tmp\n",
    "        idx_selected = np.array(idx_selected).reshape(-1)\n",
    "        features = features[self.data.idx_train][idx_selected]\n",
    "\n",
    "        # adj_knn = torch.zeros((data.nclass*args.nsamples, data.nclass*args.nsamples)).to(self.device)\n",
    "        # for i in range(data.nclass):\n",
    "        #     idx = np.arange(i*args.nsamples, i*args.nsamples+args.nsamples)\n",
    "        #     adj_knn[np.ix_(idx, idx)] = 1\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # features[features!=0] = 1\n",
    "        k = 2\n",
    "        sims = cosine_similarity(features.cpu().numpy())\n",
    "        sims[(np.arange(len(sims)), np.arange(len(sims)))] = 0\n",
    "        for i in range(len(sims)):\n",
    "            indices_argsort = np.argsort(sims[i])\n",
    "            sims[i, indices_argsort[: -k]] = 0\n",
    "        adj_knn = torch.FloatTensor(sims).to(self.device)\n",
    "        return features, adj_knn\n",
    "\n",
    "\n",
    "def get_loops(args):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if args.one_step:\n",
    "        if args.dataset =='ogbn-arxiv':\n",
    "            return 5, 0\n",
    "        return 1, 0\n",
    "    if args.dataset in ['ogbn-arxiv']:\n",
    "        return args.outer, args.inner\n",
    "    if args.dataset in ['cora']:\n",
    "        return 20, 15 # sgc\n",
    "    if args.dataset in ['citeseer']:\n",
    "        return 20, 15\n",
    "    if args.dataset in ['physics']:\n",
    "        return 20, 10\n",
    "    else:\n",
    "        return 20, 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6655786a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T03:39:30.583039Z",
     "start_time": "2023-07-07T02:33:04.199853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_syn: (446, 446) feat_syn: torch.Size([446, 500])\n",
      "[15, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwang/miniconda3/envs/ztl/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss_avg: 51.91496233258928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from gcond_transduct_multi_level import GCond 479m 622m\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# epoch 400的时候有90的ACC  alpha=0.7, beta=0.3，这个参数下模型在400轮后LOSS急剧上升！\u001b[39;00m\n\u001b[1;32m      3\u001b[0m agent \u001b[39m=\u001b[39m GCond(data, args, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m param_norms \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mGCond.train\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    261\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mnclass):\n\u001b[0;32m--> 264\u001b[0m     batch_size, n_id, adjs \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mretrieve_class_sampler(\n\u001b[1;32m    265\u001b[0m             c, adj, transductive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m    266\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mnlayers \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    267\u001b[0m         adjs \u001b[39m=\u001b[39m [adjs]\n",
      "File \u001b[0;32m~/ztl/GCond-main/utils_graphsaint.py:142\u001b[0m, in \u001b[0;36mDataGraphSAINT.retrieve_class_sampler\u001b[0;34m(self, c, adj, transductive, num, args)\u001b[0m\n\u001b[1;32m    140\u001b[0m batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_dict2[c])[:num]\n\u001b[1;32m    141\u001b[0m \u001b[39m# 使用采样器对选择的样本进行采样\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msamplers[c]\u001b[39m.\u001b[39;49msample(batch)\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/ztl/lib/python3.9/site-packages/torch_geometric/loader/neighbor_sampler.py:177\u001b[0m, in \u001b[0;36mNeighborSampler.sample\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    175\u001b[0m n_id \u001b[39m=\u001b[39m batch\n\u001b[1;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msizes:\n\u001b[0;32m--> 177\u001b[0m     adj_t, n_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madj_t\u001b[39m.\u001b[39;49msample_adj(n_id, size, replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    178\u001b[0m     e_id \u001b[39m=\u001b[39m adj_t\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mvalue()\n\u001b[1;32m    179\u001b[0m     size \u001b[39m=\u001b[39m adj_t\u001b[39m.\u001b[39msparse_sizes()[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_sparse/sample.py:36\u001b[0m, in \u001b[0;36msample_adj\u001b[0;34m(src, subset, num_neighbors, replace)\u001b[0m\n\u001b[1;32m     32\u001b[0m rowptr, col, n_id, e_id \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorch_sparse\u001b[39m.\u001b[39msample_adj(\n\u001b[1;32m     33\u001b[0m     rowptr, col, subset, num_neighbors, replace)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     value \u001b[39m=\u001b[39m value[e_id]\n\u001b[1;32m     38\u001b[0m out \u001b[39m=\u001b[39m SparseTensor(rowptr\u001b[39m=\u001b[39mrowptr, row\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol, value\u001b[39m=\u001b[39mvalue,\n\u001b[1;32m     39\u001b[0m                    sparse_sizes\u001b[39m=\u001b[39m(subset\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), n_id\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)),\n\u001b[1;32m     40\u001b[0m                    is_sorted\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m out, n_id\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from gcond_transduct_multi_level import GCond 479m 622m\n",
    "# epoch 400的时候有90的ACC  alpha=0.7, beta=0.3，这个参数下模型在400轮后LOSS急剧上升！\n",
    "agent = GCond(data, args, device='cuda:0')\n",
    "\n",
    "param_norms = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0815c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_norms[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(param_norms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 初始化输出数组\n",
    "# out_array = np.empty((len(param_norms), len(param_norms[0])))\n",
    "\n",
    "# # 迭代转换每个Tensor\n",
    "# for i, norms in enumerate(param_norms):\n",
    "#     for j, norm in enumerate(norms):\n",
    "#         out_array[i,j] = norm.cpu().detach().numpy() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 找到变化最大的参数索引\n",
    "max_idx = np.argmax([np.std(norms) for norms in param_norms])  \n",
    "\n",
    "# 绘制梯度范数变化曲线\n",
    "plt.plot(out_array[max_idx])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad Norm\")\n",
    "plt.show()\n",
    "print(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e2fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
