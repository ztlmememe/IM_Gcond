{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da5d4d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:02:11.596693Z",
     "start_time": "2023-07-07T02:02:07.621808Z"
    }
   },
   "outputs": [],
   "source": [
    "from deeprobust.graph.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from gcond_agent_transduct import GCond\n",
    "from utils_graphsaint import DataGraphSAINT\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=0, help='gpu id')\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours')\n",
    "parser.add_argument('--epochs', type=int, default=2000)\n",
    "parser.add_argument('--nlayers', type=int, default=3)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.01)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.01)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=1)\n",
    "parser.add_argument('--seed', type=int, default=15, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=1)\n",
    "parser.add_argument('--inner', type=int, default=0)\n",
    "parser.add_argument('--outer', type=int, default=20)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "import pickle\n",
    "with open('D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88119ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:02:11.657423Z",
     "start_time": "2023-07-07T02:02:11.596693Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from utils import match_loss, regularization, row_normalize_tensor\n",
    "\n",
    "import deeprobust.graph.utils as utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.gcn import GCN\n",
    "from models.sgc import SGC\n",
    "from models.sgc_multi import SGC as SGC1\n",
    "from models.parametrized_adj import PGE\n",
    "import scipy.sparse as sp\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "class GCond:\n",
    "\n",
    "    def __init__(self, data, args, device='cuda', **kwargs):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "        # n = data.nclass * args.nsamples\n",
    "        n = int(data.feat_train.shape[0] * args.reduction_rate)\n",
    "        # from collections import Counter; print(Counter(data.labels_train))\n",
    "\n",
    "        d = data.feat_train.shape[1]\n",
    "        self.nnodes_syn = n\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=device,args=args).to(device)\n",
    "\n",
    "        self.labels_syn = torch.LongTensor(self.generate_labels_syn(data)).to(device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def generate_labels_syn(self, data):\n",
    "        from collections import Counter\n",
    "        counter = Counter(data.labels_train)\n",
    "        num_class_dict = {}\n",
    "        n = len(data.labels_train)\n",
    "\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x:x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []\n",
    "        self.syn_class_indices = {}\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * self.args.reduction_rate) - sum_\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * self.args.reduction_rate), 1)\n",
    "                sum_ += num_class_dict[c]\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        self.num_class_dict = num_class_dict\n",
    "        return labels_syn\n",
    "\n",
    "\n",
    "    def test_with_val(self, verbose=True):\n",
    "        res = []\n",
    "\n",
    "        data, device = self.data, self.device\n",
    "        feat_syn, pge, labels_syn = self.feat_syn.detach(), \\\n",
    "                                self.pge, self.labels_syn\n",
    "\n",
    "        # with_bn = True if args.dataset in ['ogbn-arxiv'] else False\n",
    "        model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                    weight_decay=5e-4, nlayers=2,\n",
    "                    nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        if self.args.dataset in ['ogbn-arxiv']:\n",
    "            model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                        weight_decay=0e-4, nlayers=2, with_bn=False,\n",
    "                        nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        adj_syn = pge.inference(feat_syn)\n",
    "        args = self.args\n",
    "\n",
    "        if self.args.save:\n",
    "            torch.save(adj_syn, f'saved_ours/adj_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "            torch.save(feat_syn, f'saved_ours/feat_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "\n",
    "        if self.args.lr_adj == 0:\n",
    "            n = len(labels_syn)\n",
    "            adj_syn = torch.zeros((n, n))\n",
    "\n",
    "        model.fit_with_val(feat_syn, adj_syn, labels_syn, data,\n",
    "                     train_iters=600, normalize=True, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        labels_test = torch.LongTensor(data.labels_test).cuda()\n",
    "\n",
    "        labels_train = torch.LongTensor(data.labels_train).cuda()\n",
    "        output = model.predict(data.feat_train, data.adj_train)\n",
    "        loss_train = F.nll_loss(output, labels_train)\n",
    "        acc_train = utils.accuracy(output, labels_train)\n",
    "        if verbose:\n",
    "            print(\"Train set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_train.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_train.item()))\n",
    "        res.append(acc_train.item())\n",
    "\n",
    "        # Full graph\n",
    "        output = model.predict(data.feat_full, data.adj_full)\n",
    "        loss_test = F.nll_loss(output[data.idx_test], labels_test)\n",
    "        acc_test = utils.accuracy(output[data.idx_test], labels_test)\n",
    "        res.append(acc_test.item())\n",
    "        if verbose:\n",
    "            print(\"Test set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_test.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return res\n",
    "\n",
    "    def train(self, verbose=True):\n",
    "        args = self.args\n",
    "        data = self.data\n",
    "        feat_syn, pge, labels_syn = self.feat_syn, self.pge, self.labels_syn\n",
    "        features, adj, labels = data.feat_full, data.adj_full, data.labels_full\n",
    "        idx_train = data.idx_train\n",
    "\n",
    "        syn_class_indices = self.syn_class_indices\n",
    "\n",
    "        features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "\n",
    "        feat_sub, adj_sub = self.get_sub_adj_feat(features)\n",
    "        self.feat_syn.data.copy_(feat_sub)\n",
    "\n",
    "        if utils.is_sparse_tensor(adj):\n",
    "            adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "        else:\n",
    "            adj_norm = utils.normalize_adj_tensor(adj)\n",
    "\n",
    "        adj = adj_norm\n",
    "        adj = SparseTensor(row=adj._indices()[0], col=adj._indices()[1],\n",
    "                value=adj._values(), sparse_sizes=adj.size()).t()\n",
    "\n",
    "\n",
    "        outer_loop, inner_loop = get_loops(args)\n",
    "        loss_avg = 0\n",
    "\n",
    "        for it in range(args.epochs+1):\n",
    "            if args.dataset in ['ogbn-arxiv']:\n",
    "                model = SGC1(nfeat=feat_syn.shape[1], nhid=self.args.hidden,\n",
    "                            dropout=0.0, with_bn=False,\n",
    "                            weight_decay=0e-4, nlayers=2,\n",
    "                            nclass=data.nclass,\n",
    "                            device=self.device).to(self.device)\n",
    "            else:\n",
    "                if args.sgc == 1:\n",
    "                    model = SGC(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout,\n",
    "                                nlayers=args.nlayers, with_bn=False,\n",
    "                                device=self.device).to(self.device)\n",
    "                else:\n",
    "                    model = GCN(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout, nlayers=args.nlayers,\n",
    "                                device=self.device).to(self.device)\n",
    "\n",
    "\n",
    "            model.initialize()\n",
    "\n",
    "            model_parameters = list(model.parameters())\n",
    "\n",
    "            optimizer_model = torch.optim.Adam(model_parameters, lr=args.lr_model)\n",
    "            model.train()\n",
    "\n",
    "            for ol in range(outer_loop):\n",
    "                adj_syn = pge(self.feat_syn)\n",
    "                adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)\n",
    "                feat_syn_norm = feat_syn\n",
    "\n",
    "                BN_flag = False\n",
    "                for module in model.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    model.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = model.forward(features, adj_norm)\n",
    "                    for module in model.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                loss = torch.tensor(0.0).to(self.device)\n",
    "                for c in range(data.nclass):\n",
    "                    batch_size, n_id, adjs = data.retrieve_class_sampler(\n",
    "                            c, adj, transductive=True, args=args)\n",
    "                    if args.nlayers == 1:\n",
    "                        adjs = [adjs]\n",
    "\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    output = model.forward_sampler(features[n_id], adjs)\n",
    "                    loss_real = F.nll_loss(output, labels[n_id[:batch_size]])\n",
    "\n",
    "                    gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "                    output_syn = model.forward(feat_syn, adj_syn_norm)\n",
    "\n",
    "                    ind = syn_class_indices[c]\n",
    "                    loss_syn = F.nll_loss(\n",
    "                            output_syn[ind[0]: ind[1]],\n",
    "                            labels_syn[ind[0]: ind[1]])\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                    coeff = self.num_class_dict[c] / max(self.num_class_dict.values())\n",
    "                    loss += coeff  * match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "\n",
    "                loss_avg += loss.item()\n",
    "                # TODO: regularize\n",
    "                if args.alpha > 0:\n",
    "                    loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(labels_syn))\n",
    "                else:\n",
    "                    loss_reg = torch.tensor(0)\n",
    "\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "                # update sythetic graph\n",
    "                self.optimizer_feat.zero_grad()\n",
    "                self.optimizer_pge.zero_grad()\n",
    "                loss.backward()\n",
    "                if it % 50 < 10:\n",
    "                    self.optimizer_pge.step()\n",
    "                else:\n",
    "                    self.optimizer_feat.step()\n",
    "\n",
    "                if args.debug and ol % 5 ==0:\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                if ol == outer_loop - 1:\n",
    "                    # print('loss_reg:', loss_reg.item())\n",
    "                    # print('Gradient matching loss:', loss.item())\n",
    "                    break\n",
    "\n",
    "                feat_syn_inner = feat_syn.detach()\n",
    "                adj_syn_inner = pge.inference(feat_syn_inner)\n",
    "                adj_syn_inner_norm = utils.normalize_adj_tensor(adj_syn_inner, sparse=False)\n",
    "                feat_syn_inner_norm = feat_syn_inner\n",
    "                for j in range(inner_loop):\n",
    "                    optimizer_model.zero_grad()\n",
    "                    output_syn_inner = model.forward(feat_syn_inner_norm, adj_syn_inner_norm)\n",
    "                    loss_syn_inner = F.nll_loss(output_syn_inner, labels_syn)\n",
    "                    loss_syn_inner.backward()\n",
    "                    # print(loss_syn_inner.item())\n",
    "                    optimizer_model.step() # update gnn param\n",
    "\n",
    "\n",
    "            loss_avg /= (data.nclass*outer_loop)\n",
    "            if it % 50 == 0:\n",
    "                print('Epoch {}, loss_avg: {}'.format(it, loss_avg))\n",
    "\n",
    "            eval_epochs = [400, 600, 800, 1000, 1200, 1600, 2000, 3000, 4000, 5000]\n",
    "\n",
    "            if verbose and it in eval_epochs:\n",
    "            # if verbose and (it+1) % 50 == 0:\n",
    "                res = []\n",
    "                runs = 1 if args.dataset in ['ogbn-arxiv'] else 3\n",
    "                for i in range(runs):\n",
    "                    if args.dataset in ['ogbn-arxiv']:\n",
    "                        res.append(self.test_with_val())\n",
    "                    else:\n",
    "                        res.append(self.test_with_val())\n",
    "\n",
    "                res = np.array(res)\n",
    "                print('Train/Test Mean Accuracy:',\n",
    "                        repr([res.mean(0), res.std(0)]))\n",
    "\n",
    "    def get_sub_adj_feat(self, features):\n",
    "        data = self.data\n",
    "        args = self.args\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter;\n",
    "        counter = Counter(self.labels_syn.cpu().numpy())\n",
    "\n",
    "        for c in range(data.nclass):\n",
    "            tmp = data.retrieve_class(c, num=counter[c])\n",
    "            tmp = list(tmp)\n",
    "            idx_selected = idx_selected + tmp\n",
    "        idx_selected = np.array(idx_selected).reshape(-1)\n",
    "        features = features[self.data.idx_train][idx_selected]\n",
    "\n",
    "        # adj_knn = torch.zeros((data.nclass*args.nsamples, data.nclass*args.nsamples)).to(self.device)\n",
    "        # for i in range(data.nclass):\n",
    "        #     idx = np.arange(i*args.nsamples, i*args.nsamples+args.nsamples)\n",
    "        #     adj_knn[np.ix_(idx, idx)] = 1\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # features[features!=0] = 1\n",
    "        k = 2\n",
    "        sims = cosine_similarity(features.cpu().numpy())\n",
    "        sims[(np.arange(len(sims)), np.arange(len(sims)))] = 0\n",
    "        for i in range(len(sims)):\n",
    "            indices_argsort = np.argsort(sims[i])\n",
    "            sims[i, indices_argsort[: -k]] = 0\n",
    "        adj_knn = torch.FloatTensor(sims).to(self.device)\n",
    "        return features, adj_knn\n",
    "\n",
    "\n",
    "def get_loops(args):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if args.one_step:\n",
    "        if args.dataset =='ogbn-arxiv':\n",
    "            return 5, 0\n",
    "        return 1, 0\n",
    "    if args.dataset in ['ogbn-arxiv']:\n",
    "        return args.outer, args.inner\n",
    "    if args.dataset in ['cora']:\n",
    "        return 20, 15 # sgc\n",
    "    if args.dataset in ['citeseer']:\n",
    "        return 20, 15\n",
    "    if args.dataset in ['physics']:\n",
    "        return 20, 10\n",
    "    else:\n",
    "        return 20, 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79387d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:02:53.647081Z",
     "start_time": "2023-07-07T02:02:11.659173Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# random seed setting\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "data_graphsaint = ['flickr', 'reddit', 'ogbn-arxiv']\n",
    "if args.dataset in data_graphsaint:\n",
    "    data = DataGraphSAINT(args.dataset)\n",
    "    data_full = data.data_full\n",
    "else:\n",
    "    data_full = get_dataset(args.dataset, args.normalize_features)\n",
    "    data = Transd2Ind(data_full, keep_ratio=args.keep_ratio)\n",
    "\n",
    "agent = GCond(data, args, device='cuda')\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de6fe37b",
   "metadata": {},
   "source": [
    "Namespace(gpu_id=0, dataset='cora', dis_metric='ours', epochs=2000, nlayers=3, hidden=256, lr_adj=0.01, lr_feat=0.01, lr_model=0.01, weight_decay=0.0, dropout=0.0, normalize_features=True, keep_ratio=1.0, reduction_rate=1, seed=15, alpha=0, debug=0, sgc=1, inner=0, outer=20, save=0, one_step=0)\n",
    "size of adj_train: (140, 140)\n",
    "#edges in adj_train: 42.0\n",
    "adj_syn: (140, 140) feat_syn: torch.Size([140, 1433])\n",
    "Epoch 0, loss_avg: 1.7759368147168841\n",
    "Epoch 50, loss_avg: 0.35925117220452035\n",
    "Epoch 100, loss_avg: 0.40190719997415236\n",
    "Epoch 150, loss_avg: 0.5239663364525419\n",
    "Epoch 200, loss_avg: 0.36625773912046006\n",
    "Epoch 250, loss_avg: 0.5542276038093111\n",
    "Epoch 300, loss_avg: 0.41440569191398985\n",
    "Epoch 350, loss_avg: 0.39552807092520154\n",
    "Epoch 400, loss_avg: 0.4064208904741225\n",
    "Train set results: loss= 1.9237 accuracy= 0.3857\n",
    "Test set results: loss= 1.9327 accuracy= 0.3670\n",
    "Train set results: loss= 1.7657 accuracy= 0.4929\n",
    "Test set results: loss= 1.8355 accuracy= 0.4900\n",
    "Train set results: loss= 1.9424 accuracy= 0.1714\n",
    "Test set results: loss= 1.9357 accuracy= 0.3350\n",
    "Train/Test Mean Accuracy: [array([0.35      , 0.39733333]), array([0.13363062, 0.06681484])]\n",
    "Epoch 450, loss_avg: 0.4025551050630076\n",
    "Epoch 500, loss_avg: 0.41581059713729446\n",
    "Epoch 550, loss_avg: 0.38633845084321927\n",
    "Epoch 600, loss_avg: 0.4420967099078974\n",
    "Train set results: loss= 1.7636 accuracy= 0.5000\n",
    "Test set results: loss= 1.8466 accuracy= 0.4600\n",
    "Train set results: loss= 1.8988 accuracy= 0.4571\n",
    "Test set results: loss= 1.9184 accuracy= 0.4260\n",
    "Train set results: loss= 1.8294 accuracy= 0.4643\n",
    "Test set results: loss= 1.8796 accuracy= 0.4640\n",
    "Train/Test Mean Accuracy: [array([0.47380952, 0.45      ]), array([0.01874764, 0.01704895])]\n",
    "Epoch 650, loss_avg: 0.3903278769493428\n",
    "Epoch 700, loss_avg: 0.3665246875667153\n",
    "Epoch 750, loss_avg: 0.5070728467971913\n",
    "Epoch 800, loss_avg: 0.4146809829740915\n",
    "Train set results: loss= 1.9423 accuracy= 0.1643\n",
    "Test set results: loss= 1.9337 accuracy= 0.2970\n",
    "Train set results: loss= 1.8724 accuracy= 0.3071\n",
    "Test set results: loss= 1.8878 accuracy= 0.3350\n",
    "Train set results: loss= 1.7771 accuracy= 0.5214\n",
    "Test set results: loss= 1.9080 accuracy= 0.2830\n",
    "Train/Test Mean Accuracy: [array([0.33095238, 0.305     ]), array([0.14677176, 0.02196968])]\n",
    "Epoch 850, loss_avg: 0.5413079593074552\n",
    "Epoch 900, loss_avg: 0.47278785365451526\n",
    "Epoch 950, loss_avg: 0.447943849940348\n",
    "Epoch 1000, loss_avg: 0.4317427012531663\n",
    "Train set results: loss= 1.8790 accuracy= 0.4286\n",
    "Test set results: loss= 1.8966 accuracy= 0.4370\n",
    "Train set results: loss= 1.8771 accuracy= 0.4571\n",
    "Test set results: loss= 1.9066 accuracy= 0.3940\n",
    "Train set results: loss= 1.7819 accuracy= 0.5071\n",
    "Test set results: loss= 1.8499 accuracy= 0.4950\n",
    "Train/Test Mean Accuracy: [array([0.46428571, 0.442     ]), array([0.03247186, 0.04138438])]\n",
    "Epoch 1050, loss_avg: 0.4331214787188488\n",
    "Epoch 1100, loss_avg: 0.41300814937775837\n",
    "Epoch 1150, loss_avg: 0.3581344030698272\n",
    "Epoch 1200, loss_avg: 0.34708585351091426\n",
    "Train set results: loss= 1.7652 accuracy= 0.4286\n",
    "Test set results: loss= 1.7931 accuracy= 0.4270\n",
    "Train set results: loss= 1.8825 accuracy= 0.4357\n",
    "Test set results: loss= 1.9129 accuracy= 0.4070\n",
    "Train set results: loss= 1.7439 accuracy= 0.3643\n",
    "Test set results: loss= 1.8075 accuracy= 0.2860\n",
    "Train/Test Mean Accuracy: [array([0.40952381, 0.37333333]), array([0.0321208 , 0.06229143])]\n",
    "Epoch 1250, loss_avg: 0.36526451263980625\n",
    "Epoch 1300, loss_avg: 0.35293615582096843\n",
    "Epoch 1350, loss_avg: 0.3472937974582445\n",
    "Epoch 1400, loss_avg: 0.3611282962417761\n",
    "Epoch 1450, loss_avg: 0.3314151207832404\n",
    "Epoch 1500, loss_avg: 0.36608366020958216\n",
    "Epoch 1550, loss_avg: 0.32801023545375774\n",
    "Epoch 1600, loss_avg: 0.3332204806460924\n",
    "Train set results: loss= 1.7556 accuracy= 0.4857\n",
    "Test set results: loss= 1.8370 accuracy= 0.4770\n",
    "Train set results: loss= 1.7725 accuracy= 0.4214\n",
    "Test set results: loss= 1.8027 accuracy= 0.3990\n",
    "Train set results: loss= 1.6970 accuracy= 0.4571\n",
    "Test set results: loss= 1.7838 accuracy= 0.4490\n",
    "Train/Test Mean Accuracy: [array([0.4547619 , 0.44166667]), array([0.02629848, 0.03226281])]\n",
    "Epoch 1650, loss_avg: 0.22523460349747124\n",
    "Epoch 1700, loss_avg: 0.25224294415476156\n",
    "Epoch 1750, loss_avg: 0.2298066415084843\n",
    "Epoch 1800, loss_avg: 0.21314260149164665\n",
    "Epoch 1850, loss_avg: 0.21527705737458236\n",
    "Epoch 1900, loss_avg: 0.2286267828897535\n",
    "Epoch 1950, loss_avg: 0.24509261194839113\n",
    "Epoch 2000, loss_avg: 0.24156516746225845\n",
    "Train set results: loss= 1.7615 accuracy= 0.3500\n",
    "Test set results: loss= 1.7767 accuracy= 0.4100\n",
    "Train set results: loss= 1.7138 accuracy= 0.4214\n",
    "Test set results: loss= 1.7931 accuracy= 0.4260\n",
    "Train set results: loss= 1.7769 accuracy= 0.3714\n",
    "Test set results: loss= 1.7479 accuracy= 0.4360\n",
    "Train/Test Mean Accuracy: [array([0.38095238, 0.424     ]), array([0.02992811, 0.01070825])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
