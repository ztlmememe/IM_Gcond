{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b6a4e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.062830Z",
     "start_time": "2023-07-07T02:32:59.176098Z"
    }
   },
   "outputs": [],
   "source": [
    "from deeprobust.graph.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils_graphsaint import DataGraphSAINT\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=0, help='gpu id')\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours')\n",
    "parser.add_argument('--epochs', type=int, default=2000)\n",
    "parser.add_argument('--nlayers', type=int, default=3)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.01)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.01)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=1)\n",
    "parser.add_argument('--seed', type=int, default=15, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=1)\n",
    "parser.add_argument('--inner', type=int, default=0)\n",
    "parser.add_argument('--outer', type=int, default=20)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "import pickle\n",
    "with open('D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c436a832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.110704Z",
     "start_time": "2023-07-07T02:33:04.064828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu_id=0, dataset='cora', dis_metric='ours', epochs=2000, nlayers=3, hidden=256, lr_adj=0.01, lr_feat=0.01, lr_model=0.01, weight_decay=0.0, dropout=0.0, normalize_features=True, keep_ratio=1.0, reduction_rate=1, seed=15, alpha=0, debug=0, sgc=1, inner=0, outer=20, save=0, one_step=0)\n",
      "size of adj_train: (140, 140)\n",
      "#edges in adj_train: 42.0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# random seed setting\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "data_graphsaint = ['flickr', 'reddit', 'ogbn-arxiv']\n",
    "if args.dataset in data_graphsaint:\n",
    "    data = DataGraphSAINT(args.dataset)\n",
    "    data_full = data.data_full\n",
    "else:\n",
    "    data_full = get_dataset(args.dataset, args.normalize_features)\n",
    "    data = Transd2Ind(data_full, keep_ratio=args.keep_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c888ea80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T02:33:04.197861Z",
     "start_time": "2023-07-07T02:33:04.112699Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "# from utils import match_loss, regularization, row_normalize_tensor\n",
    "from utils import regularization, row_normalize_tensor\n",
    "import deeprobust.graph.utils as utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.gcn import GCN\n",
    "from models.sgc import SGC\n",
    "from models.sgc_multi import SGC as SGC1\n",
    "from models.parametrized_adj import PGE\n",
    "import scipy.sparse as sp\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "def distance_baseline(gwr, gws):\n",
    "    shape = gwr.shape\n",
    "    if len(shape) == 4:  # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2:  # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1:  # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return 0\n",
    "\n",
    "    dis = 1 - F.cosine_similarity(gwr, gws, dim=-1)\n",
    "    return torch.sum(dis)\n",
    "\n",
    "def distance_improved(gwr, gws):\n",
    "    shape = gwr.shape\n",
    "    if len(shape) == 4:  # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2:  # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1:  # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return 0\n",
    "\n",
    "    dis1 = 1 - F.cosine_similarity(gwr, gws, dim=-1)\n",
    "    dis2 = torch.norm(gwr - gws, dim=-1)\n",
    "    # dis3 = ((gwr - gws)**2).sum(dim=-1)\n",
    "    # dis4 = ((gwr - gws)**2).mean(dim=-1)\n",
    "    return torch.sum(dis1 + dis2)\n",
    "\n",
    "def match_loss(gw_syn, gw_real, dis_metric):\n",
    "    if dis_metric == 'baseline':\n",
    "        dis = 0.0\n",
    "        for ig in range(len(gw_real)):\n",
    "            gwr = gw_real[ig]\n",
    "            gws = gw_syn[ig]\n",
    "            dis += distance_baseline(gwr, gws)\n",
    "\n",
    "    elif dis_metric == 'improved':\n",
    "        dis = 0.0\n",
    "        for ig in range(len(gw_real)):\n",
    "            gwr = gw_real[ig]\n",
    "            gws = gw_syn[ig]\n",
    "            dis += distance_improved(gwr, gws)\n",
    "\n",
    "    elif dis_metric == 'mse':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n",
    "\n",
    "    elif dis_metric == 'cos':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = 1 - F.cosine_similarity(gw_real_vec, gw_syn_vec, dim=-1)\n",
    "\n",
    "    else:\n",
    "        exit('DC error: unknown distance function')\n",
    "\n",
    "    return dis\n",
    "\n",
    "class GCond:\n",
    "\n",
    "    def __init__(self, data, args, device='cuda', **kwargs):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "        # n = data.nclass * args.nsamples\n",
    "        n = int(data.feat_train.shape[0] * args.reduction_rate)\n",
    "        # from collections import Counter; print(Counter(data.labels_train))\n",
    "\n",
    "        d = data.feat_train.shape[1]\n",
    "        self.nnodes_syn = n\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=device,args=args).to(device)\n",
    "\n",
    "        self.labels_syn = torch.LongTensor(self.generate_labels_syn(data)).to(device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def generate_labels_syn(self, data):\n",
    "        from collections import Counter\n",
    "        counter = Counter(data.labels_train)\n",
    "        num_class_dict = {}\n",
    "        n = len(data.labels_train)\n",
    "\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x:x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []\n",
    "        self.syn_class_indices = {}\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * self.args.reduction_rate) - sum_\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * self.args.reduction_rate), 1)\n",
    "                sum_ += num_class_dict[c]\n",
    "                self.syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        self.num_class_dict = num_class_dict\n",
    "        return labels_syn\n",
    "\n",
    "\n",
    "    def test_with_val(self, verbose=True):\n",
    "        res = []\n",
    "\n",
    "        data, device = self.data, self.device\n",
    "        feat_syn, pge, labels_syn = self.feat_syn.detach(), \\\n",
    "                                self.pge, self.labels_syn\n",
    "\n",
    "        # with_bn = True if args.dataset in ['ogbn-arxiv'] else False\n",
    "        model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                    weight_decay=5e-4, nlayers=2,\n",
    "                    nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        if self.args.dataset in ['ogbn-arxiv']:\n",
    "            model = GCN(nfeat=feat_syn.shape[1], nhid=self.args.hidden, dropout=0.5,\n",
    "                        weight_decay=0e-4, nlayers=2, with_bn=False,\n",
    "                        nclass=data.nclass, device=device).to(device)\n",
    "\n",
    "        adj_syn = pge.inference(feat_syn)\n",
    "        args = self.args\n",
    "\n",
    "        if self.args.save:\n",
    "            torch.save(adj_syn, f'saved_ours/adj_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "            torch.save(feat_syn, f'saved_ours/feat_{args.dataset}_{args.reduction_rate}_{args.seed}.pt')\n",
    "\n",
    "        if self.args.lr_adj == 0:\n",
    "            n = len(labels_syn)\n",
    "            adj_syn = torch.zeros((n, n))\n",
    "\n",
    "        model.fit_with_val(feat_syn, adj_syn, labels_syn, data,\n",
    "                     train_iters=600, normalize=True, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        labels_test = torch.LongTensor(data.labels_test).cuda()\n",
    "\n",
    "        labels_train = torch.LongTensor(data.labels_train).cuda()\n",
    "        output = model.predict(data.feat_train, data.adj_train)\n",
    "        loss_train = F.nll_loss(output, labels_train)\n",
    "        acc_train = utils.accuracy(output, labels_train)\n",
    "        if verbose:\n",
    "            print(\"Train set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_train.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_train.item()))\n",
    "        res.append(acc_train.item())\n",
    "\n",
    "        # Full graph\n",
    "        output = model.predict(data.feat_full, data.adj_full)\n",
    "        loss_test = F.nll_loss(output[data.idx_test], labels_test)\n",
    "        acc_test = utils.accuracy(output[data.idx_test], labels_test)\n",
    "        res.append(acc_test.item())\n",
    "        if verbose:\n",
    "            print(\"Test set results:\",\n",
    "                  \"loss= {:.4f}\".format(loss_test.item()),\n",
    "                  \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return res\n",
    "\n",
    "    def train(self, verbose=True):\n",
    "        args = self.args\n",
    "        data = self.data\n",
    "        feat_syn, pge, labels_syn = self.feat_syn, self.pge, self.labels_syn\n",
    "        features, adj, labels = data.feat_full, data.adj_full, data.labels_full\n",
    "        idx_train = data.idx_train\n",
    "\n",
    "        syn_class_indices = self.syn_class_indices\n",
    "\n",
    "        features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "\n",
    "        feat_sub, adj_sub = self.get_sub_adj_feat(features)\n",
    "        self.feat_syn.data.copy_(feat_sub)\n",
    "\n",
    "        if utils.is_sparse_tensor(adj):\n",
    "            adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "        else:\n",
    "            adj_norm = utils.normalize_adj_tensor(adj)\n",
    "\n",
    "        adj = adj_norm\n",
    "        adj = SparseTensor(row=adj._indices()[0], col=adj._indices()[1],\n",
    "                value=adj._values(), sparse_sizes=adj.size()).t()\n",
    "\n",
    "\n",
    "        outer_loop, inner_loop = get_loops(args)\n",
    "        loss_avg = 0\n",
    "        loss_inter_avg = 0\n",
    "\n",
    "        for it in range(args.epochs+1):\n",
    "            if args.dataset in ['ogbn-arxiv']:\n",
    "                model = SGC1(nfeat=feat_syn.shape[1], nhid=self.args.hidden,\n",
    "                            dropout=0.0, with_bn=False,\n",
    "                            weight_decay=0e-4, nlayers=2,\n",
    "                            nclass=data.nclass,\n",
    "                            device=self.device).to(self.device)\n",
    "            else:\n",
    "                if args.sgc == 1:\n",
    "                    model = SGC(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout,\n",
    "                                nlayers=args.nlayers, with_bn=False,\n",
    "                                device=self.device).to(self.device)\n",
    "                else:\n",
    "                    model = GCN(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                                nclass=data.nclass, dropout=args.dropout, nlayers=args.nlayers,\n",
    "                                device=self.device).to(self.device)\n",
    "\n",
    "\n",
    "            model.initialize()\n",
    "\n",
    "            model_parameters = list(model.parameters())\n",
    "\n",
    "            optimizer_model = torch.optim.Adam(model_parameters, lr=args.lr_model)\n",
    "            model.train()\n",
    "            gws = []\n",
    "            for ol in range(outer_loop):\n",
    "                adj_syn = pge(self.feat_syn)\n",
    "                adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)\n",
    "                feat_syn_norm = feat_syn\n",
    "\n",
    "                BN_flag = False\n",
    "                for module in model.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    model.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = model.forward(features, adj_norm)\n",
    "                    for module in model.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                loss = torch.tensor(0.0).to(self.device)\n",
    "                # 多级梯度匹配实验\n",
    "                gw_real_vec = list(0.0 for _ in model_parameters)\n",
    "                gw_syn_vec = list(0.0 for _ in model_parameters)\n",
    "                \n",
    "                for c in range(data.nclass):\n",
    "                    batch_size, n_id, adjs = data.retrieve_class_sampler(\n",
    "                            c, adj, transductive=True, args=args)\n",
    "                    if args.nlayers == 1:\n",
    "                        adjs = [adjs]\n",
    "\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    output = model.forward_sampler(features[n_id], adjs)\n",
    "                    loss_real = F.nll_loss(output, labels[n_id[:batch_size]])\n",
    "\n",
    "                    gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "\n",
    "                    # 多级梯度匹配实验\n",
    "                    for i, grad in enumerate(gw_real):\n",
    "                        gw_real_vec[i] += grad\n",
    "                    \n",
    "#                     gw_real_norms = []\n",
    "#                     for g in gw_real:\n",
    "#                         g_norm = torch.norm(g, p=2)\n",
    "#                         gw_real_norms.append(g_norm)\n",
    "#                     # we can then sum or average the norms to get a single scalar\n",
    "#                     gw_real_norm = torch.sum(torch.stack(gw_real_norms))\n",
    "#                     # or\n",
    "#                     gw_real_norm += torch.mean(torch.stack(gw_real_norms))\n",
    "                # gw_real_norm is a scalar tensor representing the 2-norm of gw_rea\n",
    "                    \n",
    "\n",
    "                    output_syn = model.forward(feat_syn, adj_syn_norm)\n",
    "\n",
    "                    ind = syn_class_indices[c]\n",
    "                    loss_syn = F.nll_loss(\n",
    "                            output_syn[ind[0]: ind[1]],\n",
    "                            labels_syn[ind[0]: ind[1]])\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "\n",
    "                    for i, grad in enumerate(gw_syn):\n",
    "                        gw_syn_vec[i] += grad\n",
    "\n",
    "                    coeff = self.num_class_dict[c] / max(self.num_class_dict.values())\n",
    "                    loss += coeff  * match_loss(gw_syn, gw_real, 'improved')\n",
    "\n",
    "#                     loss += match_loss(gw_syn, gw_real, 'improved')\n",
    "\n",
    "                for i in range(len(gw_real)):\n",
    "                    gw_real_vec[i] /= self.data.nclass\n",
    "                    gw_syn_vec[i] /= self.data.nclass\n",
    "                loss += self.data.nclass * match_loss(gw_syn_vec, gw_real_vec, 'improved')\n",
    "\n",
    "#                 print(gw_real_norm/c)\n",
    "                # gws.append(gw_real_norm/c)\n",
    "                loss_avg += loss.item()\n",
    "                loss_inter_avg += (self.data.nclass * match_loss(gw_syn_vec, gw_real_vec, 'improved')).item()\n",
    "                # TODO: regularize\n",
    "                if args.alpha > 0:\n",
    "                    loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(labels_syn))\n",
    "                else:\n",
    "                    loss_reg = torch.tensor(0)\n",
    "\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "                # update sythetic graph\n",
    "                self.optimizer_feat.zero_grad()\n",
    "                self.optimizer_pge.zero_grad()\n",
    "                loss.backward()\n",
    "                if it % 50 < 10:\n",
    "                    self.optimizer_pge.step()\n",
    "                else:\n",
    "                    self.optimizer_feat.step()\n",
    "\n",
    "                if args.debug and ol % 5 ==0:\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                if ol == outer_loop - 1:\n",
    "                    # print('loss_reg:', loss_reg.item())\n",
    "                    # print('Gradient matching loss:', loss.item())\n",
    "                    break\n",
    "\n",
    "                feat_syn_inner = feat_syn.detach()\n",
    "                adj_syn_inner = pge.inference(feat_syn_inner)\n",
    "                adj_syn_inner_norm = utils.normalize_adj_tensor(adj_syn_inner, sparse=False)\n",
    "                feat_syn_inner_norm = feat_syn_inner\n",
    "                for j in range(inner_loop):\n",
    "                    optimizer_model.zero_grad()\n",
    "                    output_syn_inner = model.forward(feat_syn_inner_norm, adj_syn_inner_norm)\n",
    "                    loss_syn_inner = F.nll_loss(output_syn_inner, labels_syn)\n",
    "                    loss_syn_inner.backward()\n",
    "                    # print(loss_syn_inner.item())\n",
    "                    optimizer_model.step() # update gnn param\n",
    "\n",
    "#             list_tensor = torch.stack(gws, dim=0)\n",
    "#             # we can then use torch.topk to find the top N values and their indices\n",
    "#             # assuming N is 3 and dim is 0\n",
    "#             top_values, top_indices = torch.topk(list_tensor, k=15, dim=0)\n",
    "            # top_values is a tensor of shape [3] containing the top 3 values\n",
    "            # top_indices is a tensor of shape [3] containing the indices of the top 3 values\n",
    "#             print(top_values)\n",
    "#             print(top_indices)\n",
    "            loss_avg /= (data.nclass*outer_loop)\n",
    "            loss_inter_avg /= (data.nclass*outer_loop)\n",
    "            if it % 50 == 0:\n",
    "                print('Epoch {}, loss_avg: {}'.format(it, loss_avg))\n",
    "                print('Epoch {}, loss_inter_avg: {}'.format(it, loss_inter_avg))\n",
    "\n",
    "            eval_epochs = [400, 600, 800, 1000, 1200, 1600, 2000, 3000, 4000, 5000]\n",
    "\n",
    "            if verbose and it in eval_epochs:\n",
    "            # if verbose and (it+1) % 50 == 0:\n",
    "                res = []\n",
    "                runs = 1 if args.dataset in ['ogbn-arxiv'] else 3\n",
    "                for i in range(runs):\n",
    "                    if args.dataset in ['ogbn-arxiv']:\n",
    "                        res.append(self.test_with_val())\n",
    "                    else:\n",
    "                        res.append(self.test_with_val())\n",
    "\n",
    "                res = np.array(res)\n",
    "\n",
    "                # 这段代码是在输出训练或测试的平均准确率和标准差,准确率越高越好，标准差越小越好\n",
    "                print('Train/Test Mean Accuracy:',\n",
    "                        repr([res.mean(0), res.std(0)]))\n",
    "                # 输出示例：[array([0.91666667, 0.794 ]), array([0.02357023, 0.0008165 ])]\n",
    "                # 这个输出表明训练或测试的平均准确率为 0.91666667，标准差为 0.02357023，测试的平均准确率为 0.794，标准差为 0.0008165。\n",
    "                # 也就是说每个array的0是训练集，1是测试集，第一个array是平均准确率，第二个array是标准差\n",
    "                \n",
    "                # repr() 函数是将对象转化为供解释器读取的形式（即字符串形式），它通常用于调试和测试中，方便查看对象的值。\n",
    "\n",
    "                # res.mean(0) 计算 res 中每一列的平均值，返回一个一维张量，表示平均准确率。res.std(0) 则计算每一列的标准差，也返回一个一维张量，表示准确率的标准差。\n",
    "\n",
    "                # 最终输出的结果是一个二元组，第一个元素是平均准确率的一维张量，第二个元素是标准差的一维张量，它们都用 repr() 函数转换成字符串形式。\n",
    "\n",
    "    def get_sub_adj_feat(self, features):\n",
    "        data = self.data\n",
    "        args = self.args\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter;\n",
    "        counter = Counter(self.labels_syn.cpu().numpy())\n",
    "\n",
    "        for c in range(data.nclass):\n",
    "            tmp = data.retrieve_class(c, num=counter[c])\n",
    "            tmp = list(tmp)\n",
    "            idx_selected = idx_selected + tmp\n",
    "        idx_selected = np.array(idx_selected).reshape(-1)\n",
    "        features = features[self.data.idx_train][idx_selected]\n",
    "\n",
    "        # adj_knn = torch.zeros((data.nclass*args.nsamples, data.nclass*args.nsamples)).to(self.device)\n",
    "        # for i in range(data.nclass):\n",
    "        #     idx = np.arange(i*args.nsamples, i*args.nsamples+args.nsamples)\n",
    "        #     adj_knn[np.ix_(idx, idx)] = 1\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # features[features!=0] = 1\n",
    "        k = 2\n",
    "        sims = cosine_similarity(features.cpu().numpy())\n",
    "        sims[(np.arange(len(sims)), np.arange(len(sims)))] = 0\n",
    "        for i in range(len(sims)):\n",
    "            indices_argsort = np.argsort(sims[i])\n",
    "            sims[i, indices_argsort[: -k]] = 0\n",
    "        adj_knn = torch.FloatTensor(sims).to(self.device)\n",
    "        return features, adj_knn\n",
    "\n",
    "\n",
    "def get_loops(args):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if args.one_step:\n",
    "        if args.dataset =='ogbn-arxiv':\n",
    "            return 5, 0\n",
    "        return 1, 0\n",
    "    if args.dataset in ['ogbn-arxiv']:\n",
    "        return args.outer, args.inner\n",
    "    if args.dataset in ['cora']:\n",
    "        return 20, 15 # sgc\n",
    "    if args.dataset in ['citeseer']:\n",
    "        return 20, 15\n",
    "    if args.dataset in ['physics']:\n",
    "        return 20, 10\n",
    "    else:\n",
    "        return 20, 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6655786a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T03:39:30.583039Z",
     "start_time": "2023-07-07T02:33:04.199853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_syn: (140, 140) feat_syn: torch.Size([140, 1433])\n",
      "Epoch 0, loss_avg: 1908.4426827566965\n",
      "Epoch 0, loss_inter_avg: 1194.3089704241072\n",
      "Epoch 50, loss_avg: 2607.171956078901\n",
      "Epoch 50, loss_inter_avg: 1269.8518302740908\n",
      "Epoch 100, loss_avg: 1621.2197500254415\n",
      "Epoch 100, loss_inter_avg: 728.0614995888072\n",
      "Epoch 150, loss_avg: 1421.439447307289\n",
      "Epoch 150, loss_inter_avg: 563.0082046545226\n",
      "Epoch 200, loss_avg: 1447.4387194173257\n",
      "Epoch 200, loss_inter_avg: 582.2322507052005\n",
      "Epoch 250, loss_avg: 1392.559458638341\n",
      "Epoch 250, loss_inter_avg: 522.0358657030956\n",
      "Epoch 300, loss_avg: 1412.088368194925\n",
      "Epoch 300, loss_inter_avg: 542.4707429082374\n",
      "Epoch 350, loss_avg: 1438.6211086918559\n",
      "Epoch 350, loss_inter_avg: 578.2864257970721\n",
      "Epoch 400, loss_avg: 1379.393914765256\n",
      "Epoch 400, loss_inter_avg: 504.6511380201734\n",
      "Train set results: loss= 1.9295 accuracy= 0.3143\n",
      "Test set results: loss= 1.9769 accuracy= 0.2420\n",
      "Train set results: loss= 1.9329 accuracy= 0.2714\n",
      "Test set results: loss= 1.9411 accuracy= 0.2760\n",
      "Train set results: loss= 1.9379 accuracy= 0.3929\n",
      "Test set results: loss= 1.9367 accuracy= 0.2680\n",
      "Train/Test Mean Accuracy: [array([0.32619048, 0.262     ]), array([0.05028265, 0.01451436])]\n",
      "Epoch 450, loss_avg: 1550.445826555406\n",
      "Epoch 450, loss_inter_avg: 669.3839155039904\n",
      "Epoch 500, loss_avg: 1410.7421166548677\n",
      "Epoch 500, loss_inter_avg: 540.6283652213286\n",
      "Epoch 550, loss_avg: 1394.6316626091564\n",
      "Epoch 550, loss_inter_avg: 533.9891667847852\n",
      "Epoch 600, loss_avg: 1446.881371014613\n",
      "Epoch 600, loss_inter_avg: 602.875784224245\n",
      "Train set results: loss= 1.9073 accuracy= 0.6286\n",
      "Test set results: loss= 1.9264 accuracy= 0.5280\n",
      "Train set results: loss= 1.8771 accuracy= 0.5429\n",
      "Test set results: loss= 1.9084 accuracy= 0.4720\n",
      "Train set results: loss= 1.9275 accuracy= 0.3429\n",
      "Test set results: loss= 1.9273 accuracy= 0.4310\n",
      "Train/Test Mean Accuracy: [array([0.5047619, 0.477    ]), array([0.11971243, 0.0397576 ])]\n",
      "Epoch 650, loss_avg: 1519.6530957677285\n",
      "Epoch 650, loss_inter_avg: 688.6149798692659\n",
      "Epoch 700, loss_avg: 1548.1697354264927\n",
      "Epoch 700, loss_inter_avg: 715.6332931795062\n",
      "Epoch 750, loss_avg: 1443.4292539021546\n",
      "Epoch 750, loss_inter_avg: 669.5231609674998\n",
      "Epoch 800, loss_avg: 1542.1656855404772\n",
      "Epoch 800, loss_inter_avg: 731.7259374324203\n",
      "Train set results: loss= 1.6987 accuracy= 0.8786\n",
      "Test set results: loss= 1.8479 accuracy= 0.7370\n",
      "Train set results: loss= 1.6096 accuracy= 0.7857\n",
      "Test set results: loss= 1.8230 accuracy= 0.5690\n",
      "Train set results: loss= 1.6816 accuracy= 0.8214\n",
      "Test set results: loss= 1.8295 accuracy= 0.6100\n",
      "Train/Test Mean Accuracy: [array([0.82857143, 0.63866667]), array([0.03824376, 0.07151845])]\n",
      "Epoch 850, loss_avg: 1558.8698695492885\n",
      "Epoch 850, loss_inter_avg: 804.7696057621957\n",
      "Epoch 900, loss_avg: 1609.6592141753322\n",
      "Epoch 900, loss_inter_avg: 820.5352913427365\n",
      "Epoch 950, loss_avg: 1483.4620902443073\n",
      "Epoch 950, loss_inter_avg: 747.1539590291004\n",
      "Epoch 1000, loss_avg: 1607.1579058893399\n",
      "Epoch 1000, loss_inter_avg: 833.5184596927735\n",
      "Train set results: loss= 1.7433 accuracy= 0.8500\n",
      "Test set results: loss= 1.8639 accuracy= 0.5270\n",
      "Train set results: loss= 1.8227 accuracy= 0.7714\n",
      "Test set results: loss= 1.9123 accuracy= 0.4280\n",
      "Train set results: loss= 1.6306 accuracy= 0.7214\n",
      "Test set results: loss= 1.7901 accuracy= 0.5810\n",
      "Train/Test Mean Accuracy: [array([0.78095238, 0.512     ]), array([0.05291931, 0.06335614])]\n",
      "Epoch 1050, loss_avg: 1647.4401048643085\n",
      "Epoch 1050, loss_inter_avg: 800.5921867344509\n",
      "Epoch 1100, loss_avg: 1422.8516564137906\n",
      "Epoch 1100, loss_inter_avg: 652.9776170650499\n",
      "Epoch 1150, loss_avg: 1481.077013808379\n",
      "Epoch 1150, loss_inter_avg: 651.4922028847747\n",
      "Epoch 1200, loss_avg: 1514.0459306168718\n",
      "Epoch 1200, loss_inter_avg: 683.9531887149308\n",
      "Train set results: loss= 1.6024 accuracy= 0.8214\n",
      "Test set results: loss= 1.8303 accuracy= 0.4990\n",
      "Train set results: loss= 1.8601 accuracy= 0.7571\n",
      "Test set results: loss= 1.9282 accuracy= 0.4930\n",
      "Train set results: loss= 1.6874 accuracy= 0.8214\n",
      "Test set results: loss= 1.8224 accuracy= 0.6670\n",
      "Train/Test Mean Accuracy: [array([0.8  , 0.553]), array([0.03030458, 0.08064738])]\n",
      "Epoch 1250, loss_avg: 1544.7653941427295\n",
      "Epoch 1250, loss_inter_avg: 644.6212421496306\n",
      "Epoch 1300, loss_avg: 1479.47639720368\n",
      "Epoch 1300, loss_inter_avg: 693.761497202684\n",
      "Epoch 1350, loss_avg: 1636.5205203271867\n",
      "Epoch 1350, loss_inter_avg: 852.9518455722178\n",
      "Epoch 1400, loss_avg: 1509.931222012824\n",
      "Epoch 1400, loss_inter_avg: 743.3422889207656\n",
      "Epoch 1450, loss_avg: 1605.803951000308\n",
      "Epoch 1450, loss_inter_avg: 720.8584151273299\n",
      "Epoch 1500, loss_avg: 1609.2463127666308\n",
      "Epoch 1500, loss_inter_avg: 699.9282432858339\n",
      "Epoch 1550, loss_avg: 1442.9615386366775\n",
      "Epoch 1550, loss_inter_avg: 672.1223466906733\n",
      "Epoch 1600, loss_avg: 1523.7766168353796\n",
      "Epoch 1600, loss_inter_avg: 634.4540779372356\n",
      "Train set results: loss= 1.7183 accuracy= 0.7143\n",
      "Test set results: loss= 1.8330 accuracy= 0.5720\n",
      "Train set results: loss= 1.7019 accuracy= 0.8357\n",
      "Test set results: loss= 1.8259 accuracy= 0.6640\n",
      "Train set results: loss= 1.8170 accuracy= 0.6857\n",
      "Test set results: loss= 1.8700 accuracy= 0.5870\n",
      "Train/Test Mean Accuracy: [array([0.7452381 , 0.60766667]), array([0.06503095, 0.04030164])]\n",
      "Epoch 1650, loss_avg: 1581.051227870784\n",
      "Epoch 1650, loss_inter_avg: 725.5972572927543\n",
      "Epoch 1700, loss_avg: 1640.685223963389\n",
      "Epoch 1700, loss_inter_avg: 718.8756482324033\n",
      "Epoch 1750, loss_avg: 1252.5005324523597\n",
      "Epoch 1750, loss_inter_avg: 495.20767327500215\n",
      "Epoch 1800, loss_avg: 1348.5786807265176\n",
      "Epoch 1800, loss_inter_avg: 567.8737300963877\n",
      "Epoch 1850, loss_avg: 1242.1656375533905\n",
      "Epoch 1850, loss_inter_avg: 430.3556547160742\n",
      "Epoch 1900, loss_avg: 1294.32766619814\n",
      "Epoch 1900, loss_inter_avg: 477.7659545804296\n",
      "Epoch 1950, loss_avg: 1144.3642647160925\n",
      "Epoch 1950, loss_inter_avg: 401.11177732149974\n",
      "Epoch 2000, loss_avg: 1217.0894995075842\n",
      "Epoch 2000, loss_inter_avg: 407.038252164168\n",
      "Train set results: loss= 1.7658 accuracy= 0.7000\n",
      "Test set results: loss= 1.8893 accuracy= 0.4540\n",
      "Train set results: loss= 1.8504 accuracy= 0.5929\n",
      "Test set results: loss= 1.8759 accuracy= 0.5730\n",
      "Train set results: loss= 1.7632 accuracy= 0.6429\n",
      "Test set results: loss= 1.8472 accuracy= 0.4550\n",
      "Train/Test Mean Accuracy: [array([0.6452381, 0.494    ]), array([0.04377328, 0.05586293])]\n"
     ]
    }
   ],
   "source": [
    "# from gcond_transduct_multi_level import GCond\n",
    "agent = GCond(data, args, device='cuda')\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe974c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
